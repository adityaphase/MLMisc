# -*- coding: utf-8 -*-
"""Titanic predict NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11s0cpDrzR5dLfq5dyaTknf6F8XiW_xZ0
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import pandas as pd
import numpy as np
from sklearn import datasets
from sklearn.model_selection import train_test_split
from matplotlib import pyplot as plt
import torch.optim as optim

url = "https://raw.githubusercontent.com/livinundershadows/TitanicNN/master/titanic/train.csv"
titanic = pd.read_csv(url)
#select relevant columns
df1 = titanic[['Age', 'Fare', 'SibSp', 'Parch']]
df2 = titanic[['Survived']] #y -> people who survived
df1 = df1.to_numpy()
df2 = df2.to_numpy()

x = df1
y = df2
x = np.nan_to_num(x, nan=0)
x = torch.from_numpy(x).type(torch.FloatTensor)
y = torch.from_numpy(y).type(torch.LongTensor)
print(x.shape, y.shape)

class BinaryClassifier(nn.Module):
  def __init__(self):
    super(BinaryClassifier, self).__init__()
    self.fc1 = nn.Linear(4,3)
    self.fc2 = nn.Linear(3,4)
  #This must be implemented
  def forward(self,x):
        #Output of the first layer
        x = self.fc1(x)
        #activation
        x = torch.tanh(x)
        #This produces output
        x = self.fc2(x)
        return x
        
    #This function takes an input and predicts the class, (0 or 1)        
  def predict(self,x):
        #Apply softmax to output. 
        pred = F.softmax(self.forward(x))
        ans = []
        #Pick the class with maximum weight
        for t in pred:
            if t[0]>t[1]:
                ans.append(0)
            else:
                ans.append(1)
        return torch.tensor(ans)
model = BinaryClassifier()
print(model)

criteria = nn.CrossEntropyLoss()
optimiser = optim.Adam(model.parameters(), lr=0.01)

# lets train the nn
epochs = 1000
loss = 0
for i in range(epochs):
  yPred = model.forward(x)
  y = y.squeeze_()
  loss = criteria(yPred,y)
  if (i % 100 == 0):
    print("Loss: {}".format(loss.item()))
  optimiser.zero_grad()
  loss.backward()
  optimiser.step()

from sklearn.metrics import accuracy_score
print(accuracy_score(model.predict(x),y))

# test on data
url1 = "https://raw.githubusercontent.com/livinundershadows/TitanicNN/master/titanic/test.csv"
titanic = pd.read_csv(url1)
#select relevant columns
df11 = titanic[['Age', 'Fare', 'SibSp', 'Parch']]
df11 = df11.to_numpy()

x1 = df11
x1 = torch.from_numpy(x1).type(torch.FloatTensor)
print(x1.shape)

yp1 = model.predict(x1)
yp1 = yp1.numpy()